## Conclusion
{:#conclusion}

In related work of the RDF Stream Processing Community Group[](cite:cites dell'aglio2017on) and the field report on live Open datasets[](#fieldreport), we saw that publishing live changing resources on the Web leaves the options for polling and push-based mechanisms open. With this article, we shed some light into this topic by running a benchmark between a Server-Sent Event and polling interface. The benchmark is applied for the publication of live changing resources on the Web in general, but the results can also be used for the publication of RDF streams, which is the next step for live Open datasets following the Open Data deployment scheme(tim). In contrast with traditional HTTP benchmarks, we focused on assessing the latency on the client of an update instead of the HTTP response latency. We extend existing work[](cite:cites 6197172) where latency on the client must be as low as possible. We saw that a push mechanism is not only the best option under a high network latency, but also when the server needs to handle a high number of clients. If the latency on the client must be as low as possible, then the server CPU cost of HTTP polling with caching enabled does not outperform pushing. Data publishers can use our results, which reflect the performance of a pull and push mechanism over a single thread, to foresee when to scale their infrastructure in function of the number of clients and the expected maximum latency on the client. The application scenario that users have a maximal acceptable latency on the client (MAL) of at least 10s makes polling more scalable than pushing, although this is a theoretical number. Configuring the MAL is a task that an Open Data reuser should be able to choose and also this cannot be forced by the data publisher by setting a caching header. Because of this, caching headers should always be applied for invariant streams, but its timing should not be further than the next update. For variantly updating streams where the timing of the next update is unknown, we advice to only use micro-caching on the reverse proxy. At last, Open Data publishers should do user research to find out which MALs are likely to be used for each dataset and verify if their current infrastructure is fit for this MAL by using formula (1). For example: the vehicle position dataset from the field report has a new update available every 20s. Will users also configure their polling frequency in function of this interval?

The “timely fashion” requirement is currently only applied for each component individually (from Web stream to RDF stream and RSP query engines). In future work, we would like to investigate how this requirement can be resolved from a true user perspective (cfr. Smart City Dashboards) and how this requirement goes top-down to all the underlying components.
